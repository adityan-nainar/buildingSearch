import json
import os
from time import sleep

from dotenv import load_dotenv
from google import genai
from sentence_transformers import CrossEncoder

load_dotenv()
api_key = os.getenv("GEMINI_API_KEY")
client = genai.Client(api_key=api_key)
model = "gemini-2.0-flash"
cross_encoder = CrossEncoder("cross-encoder/ms-marco-TinyBERT-L2-v2")


def llm_rerank_individual(
    query: str, documents: list[dict], limit: int = 5
) -> list[dict]:
    scored_docs = []

    for doc in documents:
        prompt = f"""Rate how well this movie matches the search query.

Query: "{query}"
Movie: {doc.get("title", "")} - {doc.get("document", "")}

Consider:
- Direct relevance to query
- User intent (what they're looking for)
- Content appropriateness

Rate 0-10 (10 = perfect match).
Give me ONLY the number in your response, no other text or explanation.

Score:"""

        response = client.models.generate_content(model=model, contents=prompt)
        score_text = (response.text or "").strip()
        score = int(score_text)
        scored_docs.append({**doc, "individual_score": score})
        sleep(3)

    scored_docs.sort(key=lambda x: x["individual_score"], reverse=True)
    return scored_docs[:limit]


def llm_rerank_batch(query: str, documents: list[dict], limit: int = 5) -> list[dict]:
    if not documents:
        return []

    doc_map = {}
    doc_list = []
    for doc in documents:
        doc_id = doc["id"]
        doc_map[doc_id] = doc
        doc_list.append(
            f"{doc_id}: {doc.get('title', '')} - {doc.get('document', '')[:200]}"
        )

    doc_list_str = "\n".join(doc_list)

    prompt = f"""Rank these movies by relevance to the search query.

            Query: "{query}"

            Movies:
            {doc_list_str}

            Return ONLY the IDs in order of relevance (best match first). Return a valid JSON list, nothing else. For example:

            [75, 12, 34, 2, 1]
                """

    response = client.models.generate_content(model=model, contents=prompt)
    ranking_text = (response.text or "").strip()

    parsed_ids = json.loads(ranking_text)

    reranked = []
    for i, doc_id in enumerate(parsed_ids):
        if doc_id in doc_map:
            reranked.append({**doc_map[doc_id], "batch_rank": i + 1})

    return reranked[:limit]


def cross_encoder_rerank(
    query: str, documents: list[dict], limit: int = 5
) -> list[dict]:
    print("cross encoder function")
    pairs = []
    for doc in documents:
        pairs.append([query, f"{doc.get('title', '')} - {doc.get('document', '')}"])

    scores = cross_encoder.predict(pairs)

    for doc, score in zip(documents, scores):
        doc["crossencoder_score"] = float(score)

    documents.sort(key=lambda x: x["crossencoder_score"], reverse=True)
    for doc in documents:
        print(doc["title"])
        print(doc["metadata"])
    return documents[:limit]


def rerank(
    query: str, documents: list[dict], method: str = "batch", limit: int = 5
) -> list[dict]:
    print("rerank function")
    if method == "individual":
        return llm_rerank_individual(query, documents, limit)
    if method == "batch":
        return llm_rerank_batch(query, documents, limit)
    if method == "cross_encoder":
        print("cross_encoder call")
        return cross_encoder_rerank(query, documents, limit)
    else:
        return documents[:limit]

def evaluate(query, results):
    # Prepare the text for the LLM
    docs_to_score = [
        f"{res.get('title')} - {res.get('document')[:200]}" 
        for res in results
    ]

    prompt = f"""Rate how relevant each result is to this query on a 0-3 scale:
            Query: "{query}"
            Results:
            {chr(10).join(docs_to_score)}

            Scale:
            - 3: Highly relevant
            - 2: Relevant
            - 1: Marginally relevant
            - 0: Not relevant

            Return ONLY a valid JSON list of integers. Example: [3, 0, 2, 1]"""
    
    response = client.models.generate_content(model=model, contents=prompt)
    # Clean Markdown if present
    clean_text = response.text.replace("```json", "").replace("```", "").strip()
    
    try:
        scores = json.loads(clean_text)
        # Zip the scores back to the original result dictionaries
        for res, score in zip(results, scores):
            res["llm_eval_score"] = score
        return results
    except Exception as e:
        print(f"Evaluation failed: {e}")
        return results